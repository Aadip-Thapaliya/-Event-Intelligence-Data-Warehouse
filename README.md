# ðŸŽ‰ Event Intelligence Data Warehouse

A standalone ELT data warehouse that ingests live event data from the Ticketmaster API, transforms and models it using a star schema in PostgreSQL, runs automated data quality checks, and exposes KPI views ready for BI tools like Metabase.

Built as a portfolio project to demonstrate real-world data engineering skills: pipeline design, ETL/ELT, dimensional modelling, data governance, and warehouse architecture.

---

## Architecture

```
[ Ticketmaster API / CSV ]
          â†“
[ Ingestion Layer ]  â†’  raw.events  (PostgreSQL)
          â†“
[ Transform Layer ]  â†’  Staging views, dimension tables
          â†“
[ Warehouse Layer ]  â†’  Star schema (fact + dimensions)
          â†“
[ Reporting Layer ]  â†’  KPI views â†’ Metabase / any BI tool
```

---

## Features

- **ELT Pipeline** â€” Extract from API, land raw data, transform in-database with SQL
- **Star Schema** â€” `fact_events` with 4 dimension tables (date, venue, category, source)
- **SCD Type 2** â€” Slowly Changing Dimensions on venue table for historical accuracy
- **Idempotent Loads** â€” `ON CONFLICT` logic means re-running never creates duplicates
- **Automated Data Quality** â€” 5 checks logged on every run (null fields, bad dates, price consistency, duplicates, orphan facts)
- **KPI Views** â€” 6 reporting views ready to plug into Metabase
- **Scheduler** â€” Daily pipeline runs + hourly quality checks
- **CSV Fallback** â€” Run fully offline with 500 synthetic events (no API key needed)
- **Dockerised** â€” One command spins up PostgreSQL 15 + pgAdmin

---

## Project Structure

```
event_warehouse/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ api_ingestor.py       # Ticketmaster API â†’ raw.events
â”‚   â”œâ”€â”€ csv_ingestor.py       # CSV fallback + synthetic data generator
â”‚   â””â”€â”€ schema_raw.sql        # Raw landing schema + ingestion/quality logs
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ transform_events.sql  # Staging views, dimension population
â”‚   â””â”€â”€ data_quality.sql      # 5 automated quality checks
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ schema_star.sql       # Star schema DDL (fact + 4 dims)
â”‚   â””â”€â”€ load_facts.sql        # Idempotent fact table load + mark processed
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ kpi_queries.sql       # 6 KPI views for reporting layer
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py       # Full ELT orchestrator (CLI)
â”‚   â””â”€â”€ scheduler.py          # Automated daily + hourly runs
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_model.md         # Schema diagrams + design decisions
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Database | PostgreSQL 15 |
| Ingestion | Python, Requests, Ticketmaster API |
| Transform | SQL (views, CTEs, window functions) |
| Orchestration | Python, Schedule |
| Infrastructure | Docker, pgAdmin |
| Quality | Custom SQL checks logged to DB |

---

## Quick Start

### Prerequisites
- Python 3.10+
- Docker Desktop

### 1. Clone the repo
```bash
git clone https://github.com/yourusername/event-warehouse.git
cd event-warehouse
```

### 2. Configure environment
```bash
cp .env.example .env
```
Edit `.env`:
```
DB_USER=warehouse_user
DB_PASSWORD=yourpassword
DB_NAME=event_warehouse
DB_HOST=localhost
DB_PORT=5432
TICKETMASTER_API_KEY=your_key_here   # optional
```

### 3. Start the database
```bash
docker-compose up -d
```

### 4. Install dependencies
```bash
pip install -r requirements.txt
```

### 5. Run the pipeline

**With Ticketmaster API (real data):**
```bash
python scripts/run_pipeline.py --source api
```

**Without API key (synthetic data, works instantly):**
```bash
python scripts/run_pipeline.py --source csv --generate-sample
```

### 6. Explore in pgAdmin
Open `http://localhost:5050` â†’ login with `admin@event.com` / `admin`

---

## KPI Views Available

| View | Description |
|------|-------------|
| `reporting.kpi_events_by_category` | Volume + avg pricing by event category |
| `reporting.kpi_events_by_city` | Geographic distribution across cities |
| `reporting.kpi_monthly_trend` | Month-over-month event trends |
| `reporting.kpi_weekend_vs_weekday` | Day-type split with % of total |
| `reporting.kpi_pipeline_health` | Pipeline run success rates over time |
| `reporting.kpi_data_quality` | Quality check history + pass rates |

---

## Data Model

```
         dim_date          dim_category
            â”‚                   â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
            fact_events â”€â”€â”€â”€ dim_venue (SCD Type 2)
                 â”‚
            dim_source
```

Full schema documentation â†’ [`docs/data_model.md`](docs/data_model.md)

---

## CLI Options

```bash
python scripts/run_pipeline.py --help

  --source api          Pull from Ticketmaster API
  --source csv          Load from local CSV file
  --csv-path PATH       Path to CSV (default: sample_events.csv)
  --generate-sample     Auto-generate 500 synthetic events
  --skip-ingest         Run transform/load/quality steps only
```

---

## Roadmap

- [ ] dbt integration for transform layer
- [ ] Live FX rate table for currency normalisation
- [ ] Metabase dashboard screenshots
- [ ] Airflow DAG for production scheduling
- [ ] Additional API sources (Eventbrite, Meetup)

---

## Get a Free Ticketmaster API Key

1. Go to [developer.ticketmaster.com](https://developer.ticketmaster.com)
2. Click **Get Your API Key**
3. Sign up and copy your key into `.env`

Free tier: 5,000 calls/day â€” more than enough for development.

---

## Author

**Aadip Thapaliya**  
Data Science Student @ University of Europe for Applied Sciences  
[LinkedIn](https://www.linkedin.com/in/aadipthapaliya/) Â· [GitHub](https://github.com/Aadip)
